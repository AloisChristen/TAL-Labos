{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HKiQmVpon_7"
      },
      "source": [
        "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\"/>\n",
        "\n",
        "# Cours TAL - Laboratoire 6\n",
        "# Classification\n",
        "\n",
        "**Objectif**\n",
        "\n",
        "L’objectif de ce labo est de réaliser des expériences de classification de documents sous NLTK avec le\n",
        "corpus de dépêches Reuters. Le labo est à effectuer en binôme. Le labo sera jugé sur la qualité des\n",
        "expériences et sur la discussion des différentes options explorées. Vous devez remettre un notebook\n",
        "Jupyter présentant vos choix, votre code, vos résultats et les discussions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UugdPGKxvup8"
      },
      "source": [
        "## 1. Données\n",
        "les dépêches du corpus Reuters, tel qu’il est fourni par NLTK. Vous respecterez\n",
        "notamment la division en données d’entraînement (train) et données de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfWSduvkzL62",
        "outputId": "f88ab4cd-fdc2-4164-9b03-1e58ba765972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /root/nltk_data/corpora/reuters.zip\n",
            "replace /root/nltk_data/corpora/reuters/cats.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Imports et téléchargement du corpus\n",
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "nltk.download('reuters')\n",
        "\n",
        "!unzip /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora\n",
        "\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2JyLe9X0nZ8",
        "outputId": "bd1c99fd-5699-46b1-fcf8-fef74b571003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n",
            "10788\n",
            "['CHINA', 'DAILY', 'SAYS', 'VERMIN', 'EAT', '7', '-', ...]\n"
          ]
        }
      ],
      "source": [
        "# Exploration du corpus\n",
        "print(reuters.categories())\n",
        "print(len(reuters.fileids()))\n",
        "print(reuters.words(categories=['grain']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYucW6yJ64Gd",
        "outputId": "04e7cdbb-681f-47e8-d22c-5c5c102a7992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7WsvVZyQ1kJh"
      },
      "outputs": [],
      "source": [
        "# Séparation des documents d'entrainement et de test\n",
        "train_filenames= list(filter(lambda file: file.startswith(\"training\"),reuters.fileids()))\n",
        "test_filenames= list(filter(lambda file: file.startswith(\"test\"),reuters.fileids()))\n",
        "\n",
        "def prepareFile(filename):\n",
        "  words = reuters.raw(filename)\n",
        "  arrray_words = [word.lower() for word in word_tokenize(words) if word.isalpha()]\n",
        "  return (arrray_words, reuters.categories(filename))\n",
        "\n",
        "train_files = [prepareFile(train_filename) for train_filename in train_filenames]\n",
        "test_files  = [prepareFile(test_filename) for test_filename in test_filenames]\n",
        "# print(train_files[0])\n",
        "# print(test_files[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpCnQbN4wA2t"
      },
      "source": [
        "## 2. Hyper-paramètres \n",
        "veuillez étudier au moins deux hyperparamètres. Pour chacun, veuillez\n",
        "comparer au moins deux valeurs et indiquer laquelle fournit le meilleur score. Vous pourrez\n",
        "choisir parmi les hyperparamètres suivants :\n",
        "- options de prétraitement des textes : stopwords, lemmatisation, tout en minuscules.\n",
        "- options de représentation : présence/absence de mots indicateurs, nombre de mots\n",
        "indicateurs ; présence/absence/nombre de bigrammes, trigrammes ; autres traits :\n",
        "longueur de la dépêche, rapport tokens/types.\n",
        "- classifieurs et leurs paramètres : divers choix possibles (voir la documentation NLTK)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgXbuPAa3AdB"
      },
      "source": [
        "Nous avons choisis les hyperparamètres suivants : stopwords et les classifieurs NaiveBayesClassifier/DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VXWoUE2120Nc"
      },
      "outputs": [],
      "source": [
        "# définir les fonctions pour les hyperparamètres\n",
        "from nltk.corpus import stopwords\n",
        "# stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stop_words(words):\n",
        "  return list([word for word in words if word not in stops])\n",
        "\n",
        "stopwords_options = [\"stopwords\", \"no_stopwords\"]\n",
        "classifier_options = [\"NaiveBayes\", \"DecisionTree\"]\n",
        "\n",
        "fdist_stopwords = FreqDist([word for (words, cat) in train_files for word in words])\n",
        "fdist_no_stopwords = FreqDist(remove_stop_words([word for (words, cat) in train_files for word in words]))\n",
        "vocabulary = {\n",
        "    \"stopwords\": [word for (word, count) in fdist_stopwords.most_common(2000)],\n",
        "    \"no_stopwords\":[word for (word, count) in fdist_no_stopwords.most_common(2000)]\n",
        "}\n",
        "def create_embeddings(vocabulary, words):\n",
        "  embedding = {}\n",
        "  for word in vocabulary:\n",
        "    embedding[word] = word in words\n",
        "  return embedding\n",
        "\n",
        "def embedded_files(vocabulary, files):\n",
        "  return [(create_embeddings(vocabulary, file[0]), file[1]) for file in files]\n",
        "\n",
        "train_set = {}\n",
        "test_set = {}\n",
        "for option in stopwords_options:\n",
        "    train_set[option] = embedded_files(vocabulary[option], train_files)\n",
        "    test_set[option] = embedded_files(vocabulary[option], test_files)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTHnZQPvwDlR"
      },
      "source": [
        "## 3. Entraînement et 4. Optimisation\n",
        "Veuillez définir et entraîner trois classifieurs binaires : chacun prédit si une dépêche est\n",
        "étiquetée ou non avec la catégorie respective. Le premier classifieur binaire sera pour\n",
        "l’étiquette ‘money-fx’, le deuxième concernera ‘grain’, et le troisième sera pour ‘nat-gas’.\n",
        "Pour chacun des classifieurs, optimisez les hyperparamètres sans toucher aux données de test\n",
        "NLTK. Divisez les données d’entraînement NLTK en 80% train et 20% dev, et choisissez les\n",
        "options qui donnent les meilleurs scores sur dev."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "iwD8znCO3UHN"
      },
      "outputs": [],
      "source": [
        "from nltk import NaiveBayesClassifier\n",
        "import math\n",
        "from random import shuffle\n",
        "\n",
        "labels = ['money-fx', 'grain', 'nat-gas']\n",
        "\n",
        "# Maybe separate respecting proportion :p\n",
        "def separate_list(the_list, train_rate= 0.8):\n",
        "  index = math.floor(len(the_list) * train_rate)\n",
        "  shuffle(the_list)\n",
        "  return (the_list[:index], the_list[index:])\n",
        "\n",
        "def contains_label(data, label):\n",
        "  return (data[0], label in data[1])\n",
        "\n",
        "labelised_set = {\n",
        "    \"train\": {},\n",
        "    \"dev\": {},\n",
        "    \"test\": {}\n",
        "}\n",
        "\n",
        "for label in labels:\n",
        "  label_train_data = {}\n",
        "  label_dev_data = {}\n",
        "  label_test_data = {}\n",
        "  for option in stopwords_options:\n",
        "    labelised_train_files = [contains_label(train_file, label) for train_file in train_set[option]]\n",
        "    (label_train_data[option], label_dev_data[option]) = separate_list(labelised_train_files, train_rate=0.8)\n",
        "    label_test_data[option] = [contains_label(test_file, label) for test_file in test_set[option]]\n",
        "  labelised_set[\"train\"][label] = label_train_data\n",
        "  labelised_set[\"dev\"][label] = label_dev_data\n",
        "  labelised_set[\"test\"][label] = label_test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yVnCPb2U2Bbk"
      },
      "outputs": [],
      "source": [
        "from nltk.metrics.scores import recall, precision, f_measure\n",
        "def classifier_train(prepared_list, classifier_name, stopwords):\n",
        "  train_list = prepared_list[stopwords]\n",
        "  if classifier_name == \"NaiveBayes\":\n",
        "    classifier = nltk.classify.NaiveBayesClassifier.train(train_list)\n",
        "  elif classifier_name == \"DecisionTree\":\n",
        "    classifier = nltk.classify.DecisionTreeClassifier.train(train_list)\n",
        "  else:\n",
        "    print(\"Unknown classifier\")\n",
        "    return\n",
        "  return classifier\n",
        "\n",
        "def classifier_score(labelised_list, classifier, stopwords):\n",
        "  dev_list = labelised_list[stopwords]\n",
        "  predicted = set()\n",
        "  reference = set()\n",
        "  for i, (data, label) in enumerate(dev_list):\n",
        "    if label:\n",
        "      reference.add(i)\n",
        "    if classifier.classify(data):\n",
        "      predicted.add(i)\n",
        "  precision_score = precision(reference, predicted)\n",
        "  recall_score = recall(reference, predicted)\n",
        "  f1_score = f_measure(reference, predicted)\n",
        "  return {\"precision\": precision_score, \"recall\": recall_score, \"f1_score\": f1_score}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzJGcHgE_Y12",
        "outputId": "7087a10d-61b8-456f-cf82-3837e00e5820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'money-fx': {'NaiveBayes': {'stopwords': {'precision': 0.4008438818565401, 'recall': 0.8482142857142857, 'f1_score': 0.5444126074498566}, 'no_stopwords': {'precision': 0.49444444444444446, 'recall': 0.7739130434782608, 'f1_score': 0.6033898305084746}}, 'DecisionTree': {'stopwords': {'precision': 0.821917808219178, 'recall': 0.5357142857142857, 'f1_score': 0.6486486486486486}, 'no_stopwords': {'precision': 0.8648648648648649, 'recall': 0.5565217391304348, 'f1_score': 0.6772486772486772}}}, 'grain': {'NaiveBayes': {'stopwords': {'precision': 0.32231404958677684, 'recall': 0.896551724137931, 'f1_score': 0.47416413373860183}, 'no_stopwords': {'precision': 0.3471502590673575, 'recall': 0.8170731707317073, 'f1_score': 0.48727272727272725}}, 'DecisionTree': {'stopwords': {'precision': 0.8765432098765432, 'recall': 0.8160919540229885, 'f1_score': 0.8452380952380952}, 'no_stopwords': {'precision': 0.9, 'recall': 0.7682926829268293, 'f1_score': 0.8289473684210525}}}, 'nat-gas': {'NaiveBayes': {'stopwords': {'precision': 0.08163265306122448, 'recall': 1.0, 'f1_score': 0.15094339622641506}, 'no_stopwords': {'precision': 0.05982905982905983, 'recall': 0.4375, 'f1_score': 0.10526315789473684}}, 'DecisionTree': {'stopwords': {'precision': 0.8571428571428571, 'recall': 0.5, 'f1_score': 0.631578947368421}, 'no_stopwords': {'precision': 0.7777777777777778, 'recall': 0.4375, 'f1_score': 0.56}}}}\n"
          ]
        }
      ],
      "source": [
        "scores = {}\n",
        "for label in labels:\n",
        "  label_scores = {}\n",
        "  for classifier_type in classifier_options:\n",
        "    classifier_type_score = {}\n",
        "    for stopwords_option in stopwords_options:\n",
        "      classifier = classifier_train(labelised_set[\"train\"][label], classifier_type, stopwords_option)\n",
        "      classifier_type_score[stopwords_option] = classifier_score(labelised_set[\"dev\"][label], classifier, stopwords_option)\n",
        "    label_scores[classifier_type] = classifier_type_score\n",
        "  scores[label] = label_scores\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jL5Hdip-Avt",
        "outputId": "01441445-b232-4359-c530-4d2e3725989c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les meilleurs hyperparamètres pour le label money-fx sont DecisionTree et no_stopwords\n",
            "precision: 0.8648648648648649, rappel: 0.5565217391304348, f_measure: 0.6772486772486772\n",
            "Les meilleurs hyperparamètres pour le label grain sont DecisionTree et stopwords\n",
            "precision: 0.8765432098765432, rappel: 0.8160919540229885, f_measure: 0.8452380952380952\n",
            "Les meilleurs hyperparamètres pour le label nat-gas sont DecisionTree et stopwords\n",
            "precision: 0.8571428571428571, rappel: 0.5, f_measure: 0.631578947368421\n"
          ]
        }
      ],
      "source": [
        "best_hyper_parameters = {}\n",
        "\n",
        "for label in labels:\n",
        "  best_classifier = \"\"\n",
        "  best_stopwords_option = \"\"\n",
        "  best_score = {\"f1_score\": -1}\n",
        "  for classifier_option in classifier_options:\n",
        "    for stopwords_option in stopwords_options:\n",
        "      score = scores[label][classifier_option][stopwords_option]\n",
        "      if score[\"f1_score\"] > best_score[\"f1_score\"]:\n",
        "        best_score = score\n",
        "        best_classifier = classifier_option\n",
        "        best_stopwords_option = stopwords_option\n",
        "  best_hyper_parameters[label] = {\"stopwords\": best_stopwords_option, \"classifier\": best_classifier}\n",
        "  print(\"Les meilleurs hyperparamètres pour le label {} sont {} et {}\".format(label, best_classifier, best_stopwords_option))\n",
        "  print(\"precision: {}, rappel: {}, f_measure: {}\".format(best_score[\"precision\"], best_score[\"recall\"], best_score[\"f1_score\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fMoAIDqwhTL"
      },
      "source": [
        "## 5. Scores\n",
        "Veuillez donner les scores de rappel, précision et f-mesure de chacun des trois classifieurs,\n",
        "avec les meilleurs hyperparamètres, sur les données de test."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_scores = {}\n",
        "for label in labels:\n",
        "  best_param = best_hyper_parameters[label]\n",
        "  classifier = classifier_train(labelised_set[\"train\"][label], best_param[\"classifier\"], best_param[\"stopwords\"])\n",
        "  label_scores = classifier_score(labelised_set[\"test\"][label], classifier, best_param[\"stopwords\"])\n",
        "  test_scores[label] = label_scores\n",
        "print(test_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhMwn6Ltk1cd",
        "outputId": "3411a062-7649-4f93-ad72-c6fc24232040"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'money-fx': {'NaiveBayes': {'stopwords': {'precision': 0.4008438818565401, 'recall': 0.8482142857142857, 'f1_score': 0.5444126074498566}, 'no_stopwords': {'precision': 0.49444444444444446, 'recall': 0.7739130434782608, 'f1_score': 0.6033898305084746}}, 'DecisionTree': {'stopwords': {'precision': 0.821917808219178, 'recall': 0.5357142857142857, 'f1_score': 0.6486486486486486}, 'no_stopwords': {'precision': 0.8648648648648649, 'recall': 0.5565217391304348, 'f1_score': 0.6772486772486772}}}, 'grain': {'NaiveBayes': {'stopwords': {'precision': 0.32231404958677684, 'recall': 0.896551724137931, 'f1_score': 0.47416413373860183}, 'no_stopwords': {'precision': 0.3471502590673575, 'recall': 0.8170731707317073, 'f1_score': 0.48727272727272725}}, 'DecisionTree': {'stopwords': {'precision': 0.8765432098765432, 'recall': 0.8160919540229885, 'f1_score': 0.8452380952380952}, 'no_stopwords': {'precision': 0.9, 'recall': 0.7682926829268293, 'f1_score': 0.8289473684210525}}}, 'nat-gas': {'NaiveBayes': {'stopwords': {'precision': 0.08163265306122448, 'recall': 1.0, 'f1_score': 0.15094339622641506}, 'no_stopwords': {'precision': 0.05982905982905983, 'recall': 0.4375, 'f1_score': 0.10526315789473684}}, 'DecisionTree': {'stopwords': {'precision': 0.8571428571428571, 'recall': 0.5, 'f1_score': 0.631578947368421}, 'no_stopwords': {'precision': 0.7777777777777778, 'recall': 0.4375, 'f1_score': 0.56}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for label in labels:\n",
        "  print(\"Score pour le label {} sur les données de test : \".format(label))\n",
        "  print(\"precision: {}, rappel: {}, f_measure: {}\".format(test_scores[label][\"precision\"], test_scores[label][\"recall\"], test_scores[label][\"f1_score\"]))\n",
        "  print(\"Avec les hyperparamètres {} et {}\".format(best_hyper_parameters[label][\"classifier\"], best_hyper_parameters[label][\"stopwords\"]))\n",
        "  print(\"=\"* 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFfuCk1N0dS8",
        "outputId": "28955e04-19e4-4f6b-8677-8109fee8c138"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score pour le label money-fx sur les données de test : \n",
            "precision: 0.7043478260869566, rappel: 0.45251396648044695, f_measure: 0.5510204081632653\n",
            "Avec les hyperparamètres DecisionTree et no_stopwords\n",
            "====================\n",
            "Score pour le label grain sur les données de test : \n",
            "precision: 0.9302325581395349, rappel: 0.8053691275167785, f_measure: 0.8633093525179857\n",
            "Avec les hyperparamètres DecisionTree et stopwords\n",
            "====================\n",
            "Score pour le label nat-gas sur les données de test : \n",
            "precision: 0.7857142857142857, rappel: 0.36666666666666664, f_measure: 0.5\n",
            "Avec les hyperparamètres DecisionTree et stopwords\n",
            "====================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdwUBRUCwz8B"
      },
      "source": [
        "## 6. Classifieur multi-classe\n",
        "Veuillez définir un quatrième classifieur multi-classe qui assigne une étiquette parmi quatre :\n",
        "les trois choisies ci-dessus plus la catégorie ‘other’. Vous devrez nettoyer les données, car un\n",
        "petit nombre de dépêches sont annotées avec plusieurs étiquettes : dans ce cas, gardez\n",
        "seulement la première."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "YyjSA4WDJkqX"
      },
      "outputs": [],
      "source": [
        "multi_labels = labels + [\"other\"]\n",
        "monolabel_train_set = {}\n",
        "monolabel_test_set = {}\n",
        "for option in stopwords_options:\n",
        "  monolabel_train_set[option] = [(words, [labels[0]]) for (words, labels) in train_set[option]]\n",
        "  monolabel_test_set[option] = [(words, [labels[0]]) for (words, labels) in test_set[option]]\n",
        "\n",
        "\n",
        "def labelised_or_other(data, labels, other_label):\n",
        "  for label in labels:\n",
        "    if label in data[1]:\n",
        "      return (data[0], label)\n",
        "  return (data[0], other_label)\n",
        "\n",
        "multi_labelised_set = {\n",
        "    \"train\":{},\n",
        "    \"dev\": {},\n",
        "    \"test\": {}\n",
        "}\n",
        "for option in stopwords_options:\n",
        "    labelised_files = [labelised_or_other(train_file, labels, \"other\") for train_file in monolabel_train_set[option]]\n",
        "    (multi_labelised_set[\"train\"][option], multi_labelised_set[\"dev\"][option]) = separate_list(labelised_files, train_rate=0.8)\n",
        "    multi_labelised_set[\"test\"][option] = [labelised_or_other(test_file, labels, \"other\") for test_file in monolabel_test_set[option]]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_classifier_score(labelised_list, classifier, labels, stopwords):\n",
        "  dev_list = labelised_list[stopwords]\n",
        "  predicted = {}\n",
        "  reference = {}\n",
        "  labels_score = {}\n",
        "  for label in labels:\n",
        "    predicted[label] = set()\n",
        "    reference[label] = set()\n",
        "  for i, (data, label) in enumerate(dev_list):\n",
        "    reference[label].add(i)\n",
        "    predicted[classifier.classify(data)].add(i)\n",
        "  for label in labels:\n",
        "    precision_score = precision(reference[label], predicted[label])\n",
        "    recall_score = recall(reference[label], predicted[label])\n",
        "    f1_score = f_measure(reference[label], predicted[label])\n",
        "    labels_score[label] = {\"precision\": precision_score, \"recall\": recall_score, \"f1_score\": f1_score}\n",
        "  return labels_score"
      ],
      "metadata": {
        "id": "fSafP3u-muxa"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_scores = {}\n",
        "\n",
        "for classifier_type in classifier_options:\n",
        "    classifier_type_score = {}\n",
        "    for stopwords_option in stopwords_options:\n",
        "      classifier = classifier_train(multi_labelised_set[\"train\"], classifier_type, stopwords_option)\n",
        "      classifier_type_score[stopwords_option] = multi_classifier_score(multi_labelised_set[\"dev\"], classifier, multi_labels, stopwords_option)\n",
        "    multi_scores[classifier_type] = classifier_type_score\n",
        "print(multi_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M97pCDLmjPd",
        "outputId": "e535efca-2c6b-42ed-fa38-dc7e30ed356e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'NaiveBayes': {'stopwords': {'money-fx': {'precision': 0.17058823529411765, 'recall': 0.6304347826086957, 'f1_score': 0.26851851851851855}, 'grain': {'precision': 0.22413793103448276, 'recall': 0.7878787878787878, 'f1_score': 0.348993288590604}, 'nat-gas': {'precision': 0.0, 'recall': 0.0, 'f1_score': 0}, 'other': {'precision': 0.9779591836734693, 'recall': 0.8144119646498981, 'f1_score': 0.8887240356083087}}, 'no_stopwords': {'money-fx': {'precision': 0.21428571428571427, 'recall': 0.8048780487804879, 'f1_score': 0.3384615384615385}, 'grain': {'precision': 0.2962962962962963, 'recall': 0.7804878048780488, 'f1_score': 0.42953020134228187}, 'nat-gas': {'precision': 0.04878048780487805, 'recall': 0.25, 'f1_score': 0.08163265306122448}, 'other': {'precision': 0.9816147082334132, 'recall': 0.8387978142076503, 'f1_score': 0.9046040515653775}}}, 'DecisionTree': {'stopwords': {'money-fx': {'precision': 0.6666666666666666, 'recall': 0.043478260869565216, 'f1_score': 0.08163265306122448}, 'grain': {'precision': 0.84, 'recall': 0.6363636363636364, 'f1_score': 0.7241379310344828}, 'nat-gas': {'precision': None, 'recall': 0.0, 'f1_score': None}, 'other': {'precision': 0.9606815203145478, 'recall': 0.9966009517335146, 'f1_score': 0.9783116449783118}}, 'no_stopwords': {'money-fx': {'precision': 0.6666666666666666, 'recall': 0.14634146341463414, 'f1_score': 0.24}, 'grain': {'precision': 0.9583333333333334, 'recall': 0.5609756097560976, 'f1_score': 0.7076923076923076}, 'nat-gas': {'precision': None, 'recall': 0.0, 'f1_score': None}, 'other': {'precision': 0.9598948060486522, 'recall': 0.9972677595628415, 'f1_score': 0.9782244556113903}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_multi_hyper_parameters = {}\n",
        "\n",
        "def f1_measure_multi(multi_scores):\n",
        "  total = 0\n",
        "  for label in labels:\n",
        "    score = multi_scores[label][\"f1_score\"] if multi_scores[label][\"f1_score\"] != None else 0\n",
        "    total += score\n",
        "  total /= len(labels)\n",
        "  return total\n",
        "\n",
        "best_classifier = \"\"\n",
        "best_stopwords_option = \"\"\n",
        "best_score = -1\n",
        "best_label_scores = {}\n",
        "for classifier_option in classifier_options:\n",
        "  for stopwords_option in stopwords_options:\n",
        "    current_multi_score = multi_scores[classifier_option][stopwords_option]\n",
        "    if f1_measure_multi(current_multi_score) > best_score:\n",
        "      best_label_scores = current_multi_score\n",
        "      best_classifier = classifier_option\n",
        "      best_stopwords_option = stopwords_option\n",
        "best_multi_hyper_parameters = {\"stopwords\": best_stopwords_option, \"classifier\": best_classifier}\n",
        "print(\"Les meilleurs hyperparamètres pour le multiclassifier sont {} et {}\".format(best_classifier, best_stopwords_option))\n",
        "for label in multi_labels:\n",
        "  print(\"Scores pour le label {}\".format(label))\n",
        "  print(\"precision: {}, rappel: {}, f_measure: {}\".format(best_label_scores[label][\"precision\"], best_label_scores[label][\"recall\"], best_label_scores[label][\"f1_score\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXx7uR_r32v2",
        "outputId": "62b99953-9bd8-4782-c2ed-9d24378f9ec7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les meilleurs hyperparamètres pour le multiclassifier sont DecisionTree et no_stopwords\n",
            "Scores pour le label money-fx\n",
            "precision: 0.6666666666666666, rappel: 0.14634146341463414, f_measure: 0.24\n",
            "Scores pour le label grain\n",
            "precision: 0.9583333333333334, rappel: 0.5609756097560976, f_measure: 0.7076923076923076\n",
            "Scores pour le label nat-gas\n",
            "precision: None, rappel: 0.0, f_measure: None\n",
            "Scores pour le label other\n",
            "precision: 0.9598948060486522, rappel: 0.9972677595628415, f_measure: 0.9782244556113903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDwngbz8w_iw"
      },
      "source": [
        "##7. Score\n",
        "Veuillez donner les scores de rappel, précision et f-mesure de ce classifieur pour chacune des\n",
        "trois étiquettes choisies. Comment les scores se comparent-ils à ceux des trois classifieurs\n",
        "binaires ?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_classifier = classifier_train(multi_labelised_set[\"train\"], best_multi_hyper_parameters[\"classifier\"], best_multi_hyper_parameters[\"stopwords\"])\n",
        "multi_test_scores = multi_classifier_score(multi_labelised_set[\"test\"], classifier, multi_labels, best_multi_hyper_parameters[\"stopwords\"])"
      ],
      "metadata": {
        "id": "XcqSayN78OuZ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_test_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvLQ4WMn9vjF",
        "outputId": "80ae0268-3aca-42bb-f45d-94433f019989"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'grain': {'f1_score': 0.7272727272727273,\n",
              "  'precision': 0.7878787878787878,\n",
              "  'recall': 0.6753246753246753},\n",
              " 'money-fx': {'f1_score': 0.2644628099173554,\n",
              "  'precision': 0.64,\n",
              "  'recall': 0.16666666666666666},\n",
              " 'nat-gas': {'f1_score': None, 'precision': None, 'recall': 0.0},\n",
              " 'other': {'f1_score': 0.9755250824509634,\n",
              "  'precision': 0.9596994535519126,\n",
              "  'recall': 0.9918813978115072}}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Les scores de test du multiclassifier sont :\")\n",
        "for label in multi_labels:\n",
        "  print(\"Scores pour le label {}\".format(label))\n",
        "  print(\"precision: {}, rappel: {}, f_measure: {}\".format(multi_test_scores[label][\"precision\"], multi_test_scores[label][\"recall\"], multi_test_scores[label][\"f1_score\"]))\n",
        "  print(\"=\"* 20)\n",
        "print(\"Avec les hyperparamètres {} et {}\".format(best_classifier, best_stopwords_option))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1m-sNon9-m-",
        "outputId": "a8b71878-a79c-4111-98e4-bd32719c65f2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les scores de test du multiclassifier sont :\n",
            "Scores pour le label money-fx\n",
            "precision: 0.64, rappel: 0.16666666666666666, f_measure: 0.2644628099173554\n",
            "====================\n",
            "Scores pour le label grain\n",
            "precision: 0.7878787878787878, rappel: 0.6753246753246753, f_measure: 0.7272727272727273\n",
            "====================\n",
            "Scores pour le label nat-gas\n",
            "precision: None, rappel: 0.0, f_measure: None\n",
            "====================\n",
            "Scores pour le label other\n",
            "precision: 0.9596994535519126, rappel: 0.9918813978115072, f_measure: 0.9755250824509634\n",
            "====================\n",
            "Avec les hyperparamètres DecisionTree et no_stopwords\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour tous les labels, les classifiers dédiés sont meilleurs que le multiclassifier, ce qui parait logique vu qu'il est moins spécialisé.\n",
        "\n",
        "On notera que les hyperparmètres pour le multiclassifier ont été choisi en prenant le f1_score moyen \"Macro\". Ceci explique la non-performance pour le label \"nat-gas\""
      ],
      "metadata": {
        "id": "zj5u-rA8-Xnl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8y_eThExHdd"
      },
      "source": [
        "##8. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On remarque que le model \"DecisionTree\" est très souvent plus efficace, pour autant que les données soient présentes en quantité suffisante. Par contre le temps d'entrainement est lui beaucoup plus long.\n",
        "\n",
        "On remarque peu de différence avec ou sans les stopwords, même si souvent les enlever est meilleur.\n",
        "\n",
        "Un vocabulaire trop petit péjore également beaucoup les performances. Nous avions commencé par un vocabulaire de taille 500, mais le label \"nat-gas\" en patissait beaucoup."
      ],
      "metadata": {
        "id": "F_SH_hot-8WD"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TAL_Labo6_classification_Christen_Mottier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}