{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HKiQmVpon_7"
      },
      "source": [
        "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\"/>\n",
        "\n",
        "# Cours TAL - Laboratoire 5\n",
        "# Trois méthodes de désambiguïsation lexicale\n",
        "\n",
        "**Objectif**\n",
        "\n",
        "L'objectif de ce laboratoire est d'implémenter et de comparer plusieurs méthodes de désambiguïsation lexicale (en anglais, *Word Sense Disambiguation* ou WSD).  Vous utiliserez un corpus avec plusieurs milliers de phrases, chaque phrase contenant une occurrence du mot anglais *interest* annotée avec le sens que ce mot possède dans la phrase respective.  Les trois méthodes sont les suivantes (elles seront détaillées par la suite) :\n",
        "\n",
        "1. Algorithme de Lesk simplifié.\n",
        "1. Utilisation de word2vec.\n",
        "1. Classification supervisée (cours 9) utilisant des traits lexicaux :\n",
        "  1. les mots en position -1, -2, ..., et +1, +2, ..., par rapport à *interest* ;\n",
        "  1. apparition de mots indicateurs dans le voisinage de *interest*.\n",
        "\n",
        "Les méthodes (1) et (2) n'utilisent pas l'apprentissage automatique.  Elles fonctionnent selon le même principe : comparer le contexte d'une occurrence de *interest* avec chacune des définitions des sens (*synsets*) et choisir la définition la plus proche du contexte.  L’algorithme de Lesk définit la proximité comme le nombre de mots en commun, alors que word2vec la calcule comme la similarité de vecteurs.  La méthode (3) vise à classifier les occurrences de *interest*, les sens étant les classes, en utilisant comme traits les mots du contexte (ce sera de l'apprentissage supervisé)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z22UCahWooAB"
      },
      "source": [
        "## 0. Analyse des données\n",
        "\n",
        "Téléchargez le corpus *interest* depuis le [site du Prof. Ted Pedersen](http://www.d.umn.edu/~tpederse/data.html).  Il se trouve en bas de cette page.  Téléchargez l'archive ZIP marquée *original format without POS tags* et extrayez le fichier `interest-original.txt`.  Téléchargez également le fichier `README.int.txt` indiqué à la ligne au-dessus. Veuillez brièvement répondre aux questions suivantes :\n",
        "\n",
        "1. Quelles sont les URL du fichier ZIP et celle du fichier `README.int.txt` ?\n",
        "2. Quel est le format du fichier `interest-original.txt` et comment sont annotés les sens de *interest* ?  Est-ce qu'om considère le pluriel *interests* aussi ?  Que se passe-t-il si une phrase contient plusieurs occurrences du mot ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL1UOj6QooAD"
      },
      "outputs": [],
      "source": [
        "# 1. Le fichier zip: https://www.d.umn.edu/~tpederse/Data/interest-original.nopos.tar.gz et le fichier README: https://www.d.umn.edu/~tpederse/Data/README.int.txt\n",
        "# 2. Fichier texte avec des phrases séparée par des $$ et les mots interest/s avec un underscore et un id pour désigner leur sens. Le pluriel est considéré. Une seule occurence est traitée par phrase. Si la phrase comporte plusieurs occurence, la phrase sera copié et la prochaine occurence sera traîtée. Les occurences non-traitée sont précédées d'une *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u0n92wXooAE"
      },
      "source": [
        "3. D'après le fichier `README.int.txt`, quelles sont les définitions des six sens de *interest* annotés dans les données et quelles sont leurs fréquences ? Vous pouvez copier/coller l'extrait de `README`ici."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJUMn6ZTooAG"
      },
      "outputs": [],
      "source": [
        "# Sense 1 =  361 occurrences (15%) - readiness to give attention\n",
        "# Sense 2 =   11 occurrences (01%) - quality of causing attention to be given to\n",
        "# Sense 3 =   66 occurrences (03%) - activity, etc. that one gives attention to\n",
        "# Sense 4 =  178 occurrences (08%) - advantage, advancement or favor\n",
        "# Sense 5 =  500 occurrences (21%) - a share in a company or business\n",
        "# Sense 6 = 1252 occurrences (53%) - money paid for the use of money"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H7I1QUrooAH"
      },
      "source": [
        "4. De quel dictionnaire viennent les sens précédents ? Où peut-on le consulter en ligne ?  Veuillez aligner les définitions du dictionnaire avec les six sens annotés en écrivant par exemple `Sense 3 = \"an activity that you enjoy doing or a subject that you enjoy studying\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB_CgIJcooAK"
      },
      "outputs": [],
      "source": [
        "# Le dictionnaire est LDOCE (Longman Dictionary of Contemporary English)\n",
        "# Sense 1 = \"something you want to know or learn more about\"\n",
        "# Sense 2 = \"a quality or feature of something that attracts your attention or makes you want to know more about it\"\n",
        "# Sense 3 = \"an activity that you enjoy doing or a subject that you enjoy studying\"\n",
        "# Sense 4 = \"the things that bring advantages to someone or something\"\n",
        "# Sense 5 = \"if you have an interest in a particular company or industry, you own shares in it\"\n",
        "# Sense 6 = \"the extra money that you must pay back when you borrow money\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMv7ZS7DooAL"
      },
      "source": [
        "5. En consultant [WordNet en ligne](http://wordnetweb.princeton.edu/perl/webwn), trouvez les définitions des synsets  pour le **nom commun** *interest*.  Combien de synsets y a-t-il ?  Veuillez indiquer comme avant la **définition** de chaque synset pour chacun des six sens ci-dessus (au besoin, fusionner ou ignorer des synsets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKb4fM-aooAM"
      },
      "outputs": [],
      "source": [
        "# Veuillez répondre ici (en commentaire) à la question.\n",
        "# Il y a 7 synsets présents sur WordNet.\n",
        "# Sense 1 = \"a sense of concern with and curiosity about someone or something\"\n",
        "# Sense 2 = \"the power of attracting or holding one's attention (because it is unusual or exciting etc.)\"\n",
        "# Sense 3 = \"a diversion that occupies one's time and thoughts (usually pleasantly)\"\n",
        "# Sense 4 = \"a reason for wanting something done\"\n",
        "# Sense 5 = \"a right or legal share of something; a financial involvement with something\"\n",
        "# Sense 6 = \"a fixed charge for borrowing money; usually a percentage of the amount borrowed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34J5iztwooAO"
      },
      "source": [
        "6. Définissez (manuellement, ou avec quelques lignes de code) une liste nommée `senses1` avec les mots des définitions du README, en supprimant les stopwords (p.ex. les mots < 4 lettres).  Affichez la liste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oQ2M9LgCooAO"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OtfMyPSlooAP"
      },
      "outputs": [],
      "source": [
        "from random import randrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Up6mPLJfrUIC"
      },
      "outputs": [],
      "source": [
        "readme_definitions = [\n",
        "\"readiness to give attention\",\n",
        "\"quality of causing attention to be given to\",\n",
        "\"activity, etc. that one gives attention to\",\n",
        "\"advantage, advancement or favor\",\n",
        "\"a share in a company or business\",\n",
        "\"money paid for the use of money\"\n",
        "]\n",
        "dictionnary_definitions = [\n",
        "\"something you want to know or learn more about\",\n",
        "\"a quality or feature of something that attracts your attention or makes you want to know more about it\",\n",
        "\"an activity that you enjoy doing or a subject that you enjoy studying\",\n",
        "\"the things that bring advantages to someone or something\",\n",
        "\"if you have an interest in a particular company or industry, you own shares in it\",\n",
        "\"the extra money that you must pay back when you borrow money\"\n",
        "]\n",
        "synSet_definitions = [\n",
        "\"a sense of concern with and curiosity about someone or something\",\n",
        "\"the power of attracting or holding one's attention (because it is unusual or exciting etc.)\",\n",
        "\"a diversion that occupies one's time and thoughts (usually pleasantly)\",\n",
        "\"a reason for wanting something done\",\n",
        "\"a right or legal share of something; a financial involvement with something\",\n",
        "\"a fixed charge for borrowing money; usually a percentage of the amount borrowed\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjaEmWUOooAP",
        "outputId": "eaa838b2-bf52-4ee6-d886-4610573fbfd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['readiness', 'give', 'attention'], ['quality', 'causing', 'attention', 'given'], ['activity,', 'etc.', 'that', 'gives', 'attention'], ['advantage,', 'advancement', 'favor'], ['share', 'company', 'business'], ['money', 'paid', 'money']]\n"
          ]
        }
      ],
      "source": [
        "# Veuillez répondre ici à la question et créer la variable 'senses1' (liste de 6 listes de chaînes).\n",
        "\n",
        "senses1 = [[word for word in definition.split() if len(word) > 3] for definition in readme_definitions]\n",
        "\n",
        "print(senses1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpM5G0LFooAR"
      },
      "source": [
        "7. En combinant les définitions obtenues aux points (3) et (4) ci-dessus, construisez une liste nommée `senses2` avec pour chacun des sens de *interest* une liste de **mots-clés** correspondants.  Vous pouvez concaténer les définitions, puis écrire des instructions en Python pour extraire les mots (uniques).  Respectez l'ordre des sens données par `README`, et à la fin affichez `senses2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RjOWuFBMooAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b696aa0-9bfc-4047-966f-90513d8a38d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['about', 'attention', 'give', 'know', 'learn', 'more', 'readiness', 'something', 'want'], ['about', 'attention', 'attracts', 'causing', 'feature', 'given', 'know', 'makes', 'more', 'quality', 'something', 'that', 'want', 'your'], ['activity', 'activity,', 'attention', 'doing', 'enjoy', 'etc.', 'gives', 'studying', 'subject', 'that'], ['advancement', 'advantage,', 'advantages', 'bring', 'favor', 'someone', 'something', 'that', 'things'], ['business', 'company', 'have', 'industry,', 'particular', 'share', 'shares'], ['back', 'borrow', 'extra', 'money', 'must', 'paid', 'that', 'when']]\n"
          ]
        }
      ],
      "source": [
        "# Veuillez répondre ici à la question et créer la variable 'senses2' (liste de 6 listes de chaînes).\n",
        "senses2 = np.array([readme_definitions[i] + ' ' + dictionnary_definitions[i] for i in range(6)])\n",
        "\n",
        "senses2 = [[word for word in np.unique(sentence.split()) if len(word) > 3 and \"interest\" not in word] for sentence in senses2]\n",
        "\n",
        "\n",
        "print(senses2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_FCTUJcooAS"
      },
      "source": [
        "8. Chargez les données depuis `interest-original.txt` dans une liste appelée `sentences` qui contient pour chaque phrase la liste des mots (sans les séparateurs *$$* et *===...*).  Les phrases sont-elles déjà tokenisées ?  Sinon, faites-le.  À ce stade, ne modifiez pas encore les occurrences annotées *interest(s)\\_X*.  Comptez le nombre total de phrases et affichez-en trois au hasard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2C2hkw3ooAT",
        "outputId": "e3ca57f0-3dfb-4457-a51f-f9df0a1db259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Il y a 2368 phrases.\n",
            "En voici 3 au hasard :\n",
            "[['investor', 'interest_1', 'in', 'stock', 'funds', 'has', 'nt', 'stalled', 'at', 'all', 'mr', 'hines', 'maintains'], ['it', 'is', 'in', 'the', 'western', 'interest_4', 'to', 'see', 'mr', 'gorbachev', 'succeed'], ['revco', 'insists', 'that', 'the', 'proposal', 'is', 'simply', 'an', 'expression', 'of', 'interest_1', 'because', 'under', 'chapter', '11', 'revco', 'has', 'exclusivity', 'rights', 'until', 'feb', '28']]\n"
          ]
        }
      ],
      "source": [
        "# Veuillez répondre ici à la question.\n",
        "#Les phrases sont déjà tokenisées\n",
        "import re\n",
        "sentences = []\n",
        "with open(\"interest-original.txt\", 'r') as f:\n",
        "  current_line = \"\"\n",
        "  for line in f.readlines():\n",
        "      if line == \"$$\\n\":\n",
        "        sentences.append(re.sub(\"  \", \" \", current_line).split())\n",
        "        current_line = \"\"\n",
        "      else:\n",
        "        current_line += re.sub(r\"[^\\w ]*\", \"\", line)\n",
        "\n",
        "\n",
        "print(\"Il y a {} phrases.\\nEn voici 3 au hasard :\".format(len(sentences)))\n",
        "print(sentences[151:154])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlpMecE2ooAT"
      },
      "source": [
        "## 1. Algorithme de Lesk simplifié\n",
        "\n",
        "Définissez une fonction `wsd_lesk(senses, sentence)` qui prend deux arguments : une liste de listes de mots-clés (comme `senses1` et `senses2` ci-dessus) et une phrase avec une occurrence annotée de *interest* ou *interests*, et qui retourne l'index du sens le plus probable (entre 1 et 6) selon l'algorithme de Lesk.  Cet algorithme choisit le sens qui a le maximum de mots en commun avec le contexte de *interest*.  Vous pouvez choisir vous-mêmes la taille de ce voisinage (`window_size`).  En cas d'égalité entre deux sens, tirer la réponse au sort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PpkYGvRDooAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a22d49-379e-4d87-fa67-0081c4ed251f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "# Veuillez répondre ici à la question.\n",
        "from nltk.wsd import lesk\n",
        "import random\n",
        "\n",
        "def get_index_of_regexp_match(regexp, words):\n",
        "  for index, value in enumerate(words):\n",
        "    if re.match(regexp, value):\n",
        "      return index\n",
        "  return -1\n",
        "\n",
        "def get_index_sens(words):\n",
        "  return get_index_of_regexp_match(r\"^.*_\\d$\", words)\n",
        "\n",
        "def wsd_lesk(senses, sentence, window_size = 100):\n",
        "  word_index = get_index_sens(sentence)\n",
        "  is_plural = \"interests\" in sentence[word_index]\n",
        "  start = max(word_index - window_size, 0)\n",
        "  end = min(word_index + window_size + 1, len(sentence))\n",
        "  windowed_sentence = sentence[start:end]\n",
        "\n",
        "  maximum_count = -1\n",
        "  senses_with_maximum_count = []\n",
        "  for index, sens in enumerate(senses):\n",
        "    count = len(list(word for word in sens if word in windowed_sentence))\n",
        "    if count > maximum_count:\n",
        "      maximum_count = count\n",
        "      senses_with_maximum_count = [index]\n",
        "    elif count == maximum_count:\n",
        "      senses_with_maximum_count.append(index)\n",
        "  random.shuffle(senses_with_maximum_count)\n",
        "  return senses_with_maximum_count[0] + 1\n",
        "\n",
        "print(wsd_lesk(senses2, sentences[149]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGAXYtwNooAU"
      },
      "source": [
        "Définissez maintenant une fonction `evaluate_wsd(fct_name, senses, sentences)` qui prend en paramètre le nom de la méthode de similarité (pour commencer : `wsd_lesk`) ainsi que la liste des mots-clés par sens, et la liste de phrases, et qui retourne le score de la méthode de similarité.  Ce score sera tout simplement le pourcentage de réponses correctes (sens trouvé identique au sens annoté)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "I6ic_w6TooAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f33d3a6-075c-4970-82a7-9ef6a0c82a05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le meilleur score est 21.397804054054053 avec le sens 2 pour un voisinnage de 7\n"
          ]
        }
      ],
      "source": [
        "# Veuillez répondre ici à la question.\n",
        "def evaluate_wsd(fct_name, senses, sentences, window_size = 30):\n",
        "  if fct_name == \"wsd_lesk\":\n",
        "    score = 0\n",
        "    for sentence in sentences:\n",
        "      index = get_index_sens(sentence)\n",
        "      sens_nb = int(sentence[index][-1])\n",
        "      if sens_nb == wsd_lesk(senses, sentence, window_size= window_size):\n",
        "        score += 1\n",
        "    return score * 100 / len(sentences)\n",
        "  elif fct_name == \"wsd_word2vec\":\n",
        "    score = 0\n",
        "    for sentence in sentences:\n",
        "      index = get_index_sens(sentence)\n",
        "      sens_nb = int(sentence[index][-1])\n",
        "      if sens_nb == wsd_word2vec(senses, sentence, window_size= window_size):\n",
        "        score += 1\n",
        "    return score * 100 / len(sentences)\n",
        "  else:\n",
        "    print(\"Function {} not implemented\".format(fct_name))\n",
        "\n",
        "def best_score(fct_name, max_window_size):\n",
        "  best_score = 0\n",
        "  best_window = 0\n",
        "  best_sens = 0\n",
        "  for window_size in range(max_window_size):\n",
        "    score = 0\n",
        "    for i in range(10):\n",
        "      score += evaluate_wsd(fct_name, senses2, sentences, window_size= window_size)\n",
        "    score /= 10\n",
        "    if score > best_score:\n",
        "      best_score = score\n",
        "      best_window = window_size\n",
        "      best_sens = 2\n",
        "    score = 0\n",
        "    for i in range(10):\n",
        "      score += evaluate_wsd(fct_name, senses1, sentences, window_size= window_size)\n",
        "    score /= 10\n",
        "    if score > best_score:\n",
        "      best_score = score\n",
        "      best_window = window_size\n",
        "      best_sens = 1\n",
        "  return (best_score, best_sens, best_window)\n",
        "lesk_score = best_score(\"wsd_lesk\", 10)\n",
        "print(\"Le meilleur score de Lesk est {} avec le sens {} pour un voisinnage de {}\".format(lesk_score[0], lesk_score[1], lesk_score[2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy7QEfwbooAV"
      },
      "source": [
        "En fixant au mieux la taille de la fenêtre autour de *interest*, quel est le meilleur score de la méthode de Lesk simplifiée ?  Quelle liste de sens conduit à de meilleurs scores, `senses1` ou `senses2` ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieDU_4zcooAV"
      },
      "outputs": [],
      "source": [
        "# Le meilleur score est de 24%, avec senses2 et un voisinnage de 35. On suppose que la taille du voisinnage vient\n",
        "# du peu de mots présents dans les senses, et il est très probable qu'aucun mot en commun ne soit trouvé avec aucun des 6 sens,\n",
        "# et donc un sens au hasard est choisit. Augmenter le voisinage augmente les chances d'avoir un mot commun\n",
        "# A noter que les scores varie, puisque les sens données sont aléatoires en cas d'égalité"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhsM6SyaooAW"
      },
      "source": [
        "## 2. Utilisation de word2vec pour la similarité contexte vs. synset\n",
        "\n",
        "En réutilisant une partie du code de `wsd_lesk`, définissez maintenant une fonction `wsd_word2vec(senses, sentence)` qui choisit le sens en utilisant la similarité **word2vec**.  On vous encourage à chercher dans la [documentation des KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html) comment calculer directement la similarité entre deux listes de mots.\n",
        "\n",
        "Comme `wsd_lesk`, la nouvelle fonction `wsd_word2vec` prend en argument une liste de listes de mots-clés par sens (comme `senses1` et `senses2` ci-dessus), et une phrase avec une occurrence annotée de *interest* ou *interests*.  La fonction retourne le numéro du sens le plus probable selon la similarité word2vec entre les mots du sens et ceux du voisinage de *interest*. Vous pouvez choisir la taille de ce voisinage (`window_size`).  En cas d'égalité, tirer le sens au sort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fl22sRhSooAW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "de3c5769-0741-48f5-d7a5-0c0cd6b69fbd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-cda2ce209733>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpath_to_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"word2vec-google-news-300.gz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# C bin format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[0;34m(text, encoding, errors)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader\n",
        "\n",
        "# word2vec_gfile = gensim.downloader.load(\"word2vec-google-news-300\")\n",
        "# vector = word2vec_gfile.wv\n",
        "# vector.save(\"word2vec-google-news-300.gz\")\n",
        "\n",
        "path_to_model = \"word2vec-google-news-300.gz\"\n",
        "vector = gensim.models.KeyedVectors.load_word2vec_format(path_to_model, binary=True)  # C bin format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Lu12bNhyooAX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "96595eb1-10d7-49b9-b16a-5b98ac099702"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-b1c0b04b2c57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msenses_with_maximum_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mword2vec_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wsd_word2vec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Le meilleur score de Word2Vec est {} avec le sens {} pour un voisinnage de {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-79840a36c8c8>\u001b[0m in \u001b[0;36mbest_score\u001b[0;34m(fct_name, max_window_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mevaluate_wsd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfct_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msenses2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-79840a36c8c8>\u001b[0m in \u001b[0;36mevaluate_wsd\u001b[0;34m(fct_name, senses, sentences, window_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_index_sens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0msens_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0msens_nb\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mwsd_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msenses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-b1c0b04b2c57>\u001b[0m in \u001b[0;36mwsd_word2vec\u001b[0;34m(senses, sentence, window_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mmaximum_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0msenses_with_maximum_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mwindowed_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindowed_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msenses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0msens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-b1c0b04b2c57>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mmaximum_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0msenses_with_maximum_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mwindowed_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindowed_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msenses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0msens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vector' is not defined"
          ]
        }
      ],
      "source": [
        "# Veuillez répondre ici à la question.\n",
        "def extract_window_words(sentence, window_size):\n",
        "  word_index = get_index_sens(sentence)\n",
        "  start = max(word_index - window_size, 0)\n",
        "  end = min(word_index + window_size + 1, len(sentence))\n",
        "  return sentence[start:end]\n",
        "\n",
        "def wsd_word2vec(senses, sentence, window_size = 100):\n",
        "  word_index = get_index_sens(sentence)\n",
        "  is_plural = \"interests\" in sentence[word_index]\n",
        "  windowed_sentence = extract_window_words(sentence, window_size)\n",
        "\n",
        "  maximum_count = -1\n",
        "  senses_with_maximum_count = []\n",
        "  windowed_sentence = list(filter(lambda x: x in vector.vocab, windowed_sentence))\n",
        "  for index, sens in enumerate(senses):\n",
        "    sens = list(filter(lambda x: x in vector.vocab, sens))\n",
        "    count = vector.n_similarity(sens,windowed_sentence)\n",
        "    if count > maximum_count:\n",
        "      maximum_count = count\n",
        "      senses_with_maximum_count = [index]\n",
        "    elif count == maximum_count:\n",
        "      senses_with_maximum_count.append(index)\n",
        "  random.shuffle(senses_with_maximum_count)\n",
        "\n",
        "  return senses_with_maximum_count[0] + 1\n",
        "\n",
        "word2vec_score = best_score(\"wsd_word2vec\", 40)\n",
        "print(\"Le meilleur score de Word2Vec est {} avec le sens {} pour un voisinnage de {}\".format(word2vec_score[0], word2vec_score[1], word2vec_score[2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeAxZGHSooAX"
      },
      "source": [
        "Appliquez maintenant la même méthode `evaluate_wsd` avec la fonction `wsd_word2vec` (en cherchant une bonne valeur de la taille de la fenêtre) et affichez le score de la similarité word2vec.  Comment se compare-t-il avec les précédents ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oZwKacMooAY"
      },
      "outputs": [],
      "source": [
        "# Le meilleur score est de 30%, avec senses1 et un voisinnage le plus grand possible. \n",
        "# Les résultats sont meilleurs qu'avec notre algorithme de Lesk simplifié."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKKg_JOnooAY"
      },
      "source": [
        "## 3. Classification supervisée avec des traits lexicaux\n",
        "Dans cette partie du labo, vous entraînerez des classifieurs pour prédire le sens d'une occurrence dans une phrase.  Le principal défi sera de transformer chaque phrase en un ensemble de traits, pour créer les données en vue des expériences de classification.\n",
        "\n",
        "Vous utiliserez le classifieur `NaiveBayesClassifier` fourni par NLTK.  Le mode d'emploi se trouve dans le [Chapitre 6, sections 1.1-1.3](https://www.nltk.org/book/ch06.html) du livre NLTK.  Consultez-le attentivement pour trouver comment formater les données.  (Il existe de nombreux autres classifieurs supervisés, par exemple dans la boîte à outils `scikit-learn`.)\n",
        "\n",
        "De plus, vous devrez séparer les 2368 occurrences en ensembles d'entraînement et de test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbCoIis5ooAY"
      },
      "source": [
        "### 3.A. Traits lexicaux positionnels\n",
        "\n",
        "Dans cette première représentation des traits, vous les coderez comme `mot-2`, `mot-1`, `mot+1`, `mot+2`, etc. (fenêtre de taille `2*window_size` autour de *interest*) et vous leur donnerez les valeurs des mots observés aux emplacements respectifs, ou alors `NONE` si la fenêtre dépasse la limite de la phrase.  Vous ajouterez un trait qui est le mot *interest* lui-même, qui peut être au singulier ou au pluriel.  Pour chaque occurrence de *interest*, vous devez donc générer une représentation formelle avec un dictionnaire Python suivi de l'index du sens :\n",
        "```\n",
        "[{'word-1': 'in', 'word+1': 'rates', 'word-2': 'declines', 'word+2': 'NONE', 'word0': 'interest'}, 6]\n",
        "```\n",
        "L'index du sens servira à l'entraînement, puis elle sera cachée à l'évaluation, et la prédiction du système sera comparée à elle pour dire si elle est correcte ou non.  Vous regrouperez toutes ces entrées dans une liste totale de 2368 éléments appelée `items_with_features_A`.\n",
        "\n",
        "En partant de la liste des phrases appelée `sentences`(préparée plus haut), veuillez générer ici cette liste, en vous aidant si nécessaire du livre NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH0M-VtBooAY",
        "outputId": "6f4a79c0-c38c-41e6-9f14-73bdd33a1d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2368\n",
            "[[{'word-2': 'NONE', 'word-1': 'investor', 'word0': 'interest', 'word+1': 'in', 'word+2': 'stock'}, '1'], [{'word-2': 'the', 'word-1': 'western', 'word0': 'interest', 'word+1': 'to', 'word+2': 'see'}, '4'], [{'word-2': 'expression', 'word-1': 'of', 'word0': 'interest', 'word+1': 'because', 'word+2': 'under'}, '1']]\n"
          ]
        }
      ],
      "source": [
        "# Veuillez répondre ici à la question.\n",
        "\n",
        "def create_dict_around(sentence, index):\n",
        "  end = len(sentence)\n",
        "  [word,sense] = sentence[index].split(\"_\")\n",
        "  neighbours = {}\n",
        "  neighbours['word-2'] = sentence[index - 2] if index >= 2  else 'NONE'\n",
        "  neighbours['word-1'] = sentence[index - 1] if index >= 1  else 'NONE'\n",
        "  neighbours['word0']  =  word\n",
        "  neighbours['word+1'] = sentence[index + 1] if index < end - 1  else 'NONE'\n",
        "  neighbours['word+2'] = sentence[index + 2] if index < end - 2  else 'NONE'\n",
        "  return [neighbours , sense]\n",
        "\n",
        "def extract_feature_A(sentence):\n",
        "  index = get_index_sens(sentence)\n",
        "  return create_dict_around(sentence, index)\n",
        "\n",
        "items_with_features_A = [extract_feature_A(sent) for sent in sentences]\n",
        "print(len(items_with_features_A))\n",
        "print(items_with_features_A[151:154])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1E0i7CBooAZ"
      },
      "source": [
        "On souhaite maintenant entraîner un classifieur sur une partie des données, et le tester sur une autre.  Typiquement, on peut garder 80% des données pour l'entraînement et utiliser les 20% restants pour l'évaluation.  Veuillez faire cette division séparément pour chaque sens, pour que les deux ensembles contiennent les mêmes proportions de sens que l'ensemble de départ (\"stratification\"), et enregistrer les deux sous-ensembles de `items_with_features_A` sous les noms respectifs de `iwf_A_train` et `iwf_A_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bR0L3SQvooAa"
      },
      "outputs": [],
      "source": [
        "from random import shuffle\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "y7hF-zzZooAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a65bec52-0725-4eee-8bec-70ec4c5c476c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1891   477\n",
            "[[{'word-2': 'company', 'word-1': 'an', 'word0': 'interest', 'word+1': 'in', 'word+2': '2100'}, '5'], [{'word-2': 'NONE', 'word-1': 'net', 'word0': 'interest', 'word+1': 'income', 'word+2': 'in'}, '6']] [[{'word-2': 'substantial', 'word-1': 'minority', 'word0': 'interest', 'word+1': 'in', 'word+2': 'landini'}, '5'], [{'word-2': 'a', 'word-1': 'dividend', 'word0': 'interest', 'word+1': 'in', 'word+2': 'eds'}, '5']]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Veuillez répondre ici à la question.\n",
        "def separate_list(the_list, train_rate= 0.8):\n",
        "  index = math.floor(len(the_list) * train_rate)\n",
        "  shuffle(the_list)\n",
        "  return (the_list[:index], the_list[index:])\n",
        "\n",
        "\n",
        "def separate_sentence_by_sens(items, nb_senses=6, indexed0 = False):\n",
        "  separated = list([] for s in range(nb_senses))\n",
        "  for item in items:\n",
        "    sens_nb = int(item[1]) - 1\n",
        "    separated[sens_nb].append(item)\n",
        "  return separated\n",
        "\n",
        "def make_train_test(items, train_rate= 0.8):\n",
        " train = []\n",
        " test = []\n",
        " separated = separate_sentence_by_sens(items)\n",
        " for sens in separated:\n",
        "   (sens_train, sens_test) = separate_list(sens, train_rate = train_rate)\n",
        "   train += sens_train\n",
        "   test += sens_test\n",
        " random.shuffle(train)\n",
        " random.shuffle(test)\n",
        " return (train, test)\n",
        "\n",
        "(iwf_A_train, iwf_A_test) = make_train_test(items_with_features_A)\n",
        "\n",
        "print(len(iwf_A_train), ' ', len(iwf_A_test))\n",
        "print(iwf_A_test[:2], iwf_A_test[-2:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0uluTn_ooAb"
      },
      "source": [
        "Veuillez créer une instance de `NaiveBayesClassifier`, l'entraîner sur `iwf_A_train` et la tester sur `iwf_A_train` (voir la documentation NLTK).  En expérimentant avec différentes largeurs de fenêtres, quel est le meilleur score global que vous obtenez (avec la fonction `accuracy`), et comment se compare-t-il avec les précédents ?  Quels sont les traits les plus informatifs (voir la doc NLTK), et pouvez-vous expliquer cet affichage ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7i3k_bgsooAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2e1000-eeff-4d22-cd8a-f3f7311c54e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8763102725366876\n",
            "Most Informative Features\n",
            "                   word0 = 'interests'         3 : 1      =     97.1 : 1.0\n",
            "                  word+1 = 'in'                5 : 6      =     74.2 : 1.0\n",
            "                  word-1 = 'other'             3 : 6      =     45.2 : 1.0\n",
            "                  word+1 = 'of'                4 : 6      =     27.9 : 1.0\n",
            "                  word-1 = 'the'               4 : 5      =     20.9 : 1.0\n",
            "                  word+2 = 'and'               6 : 1      =     19.7 : 1.0\n",
            "                  word+2 = 'on'                6 : 5      =     19.0 : 1.0\n",
            "                  word-1 = 'own'               4 : 6      =     18.8 : 1.0\n",
            "                  word-1 = 'in'                6 : 5      =     18.2 : 1.0\n",
            "                  word-2 = 'NONE'              6 : 4      =     15.3 : 1.0\n",
            "                  word+2 = 'to'                6 : 1      =     13.9 : 1.0\n",
            "                  word-2 = 'have'              1 : 6      =     13.5 : 1.0\n",
            "                  word-1 = 'business'          3 : 5      =     12.7 : 1.0\n",
            "                  word+2 = 'the'               5 : 3      =     12.4 : 1.0\n",
            "                  word+1 = 'are'               3 : 6      =     12.2 : 1.0\n"
          ]
        }
      ],
      "source": [
        "from nltk.classify import naivebayes \n",
        "# Veuillez répondre ici à la question.\n",
        "classifier_A = nltk.NaiveBayesClassifier.train(iwf_A_train)\n",
        "print(nltk.classify.accuracy(classifier_A, iwf_A_test))\n",
        "\n",
        "classifier_A.show_most_informative_features(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les trains les plus informatifs représentent les traits qui permettent de mieux séparer 2 sens. Les informations affichées sont, dans l'ordre : \n",
        "\n",
        "1.   La position du trait et sa valeur\n",
        "2.   Les sens que le trait permet de séparer\n",
        "3.   La proportion de séparation du trait (sensA : sensB)\n",
        "\n",
        "\n",
        "Par exemple, on voit que la présence du mot \"in\" juste après le mot \"interest\" permet de séparer les sens 5 et 6, avec 73.8 plus de chance que le sens soit le n°5\n",
        "Il est intéressant de remarquer que le mot \"interests\" au pluriel sert énormément dans la séparation des sens 3 et 1.\n",
        "\n",
        "On remarque que certains sens sont plus faciles à séparer que d'autre : par exemple les sens 5 et 6 sont présent 6 fois dans les 15 traits les plus informatifs. Il serait intéressant d'avoir les traits les plus informatifs pour chaque couple de sens."
      ],
      "metadata": {
        "id": "gt0GWkdVGZ4G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaRmHNV8ooAb"
      },
      "outputs": [],
      "source": [
        "# Question supprimée\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3i_5clqooAb"
      },
      "source": [
        "### 3.B. Présence de mots indicateurs\n",
        "\n",
        "Une deuxième façon d'encoder les traits lexicaux est de constituer un vocabulaire avec les mots qui apparaissent dans le voisinage de *interest* et de définir ces mots comme traits.  Par conséquent, pour chaque occurrence de *interest*, vous allez extraire la valeur de ces traits sous la forme :\n",
        "```\n",
        "[{('rate' : True), ('in' : False), ...}, 1]\n",
        "```\n",
        "où *'rate'*, *'in'* sont les mots du vocabulaire, True/False indiquent leur présence/absence autour de l'occurrence de *interest* qui est décrite, et le dernier nombre est le sens, entre 1 et 6.\n",
        "\n",
        "Pour commencer, en partant de `sentences` et en fixant la taille de la fenêtre, veuillez constituer la liste de tous les mots observés autour de tous les voisinages de toutes les occurrences de *interest*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "zt4iYpCEooAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "533f9061-f4ab-43d7-e62b-1bd319a15778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38583\n",
            "['slide', 'amid', 'signs', 'that', 'portfolio', 'managers', 'expect', 'further', 'declines', 'in', 'interest', 'rates', 'longer', 'maturities', 'are', 'thought', 'to', 'indicate', 'declining', 'interest', 'rates', 'because', 'they', 'permit', 'portfolio', 'managers', 'to', 'retain', 'relatively', 'higher', 'before', 'they', 'blip', 'down', 'because', 'of', 'recent', 'rises', 'in', 'shortterm', 'interest', 'rates', 'vice', 'chairman', 'of', 'wr', 'grace', 'co', 'which', 'holds']\n"
          ]
        }
      ],
      "source": [
        "word_list = []\n",
        "# Veuillez répondre ici à la question.\n",
        "windowed_sentences = list(extract_window_words(sentence, 10) for sentence in sentences)\n",
        "for sentence in windowed_sentences:\n",
        "  for word in sentence:\n",
        "    word_list.append(word.split(\"_\")[0])\n",
        "print(len(word_list))\n",
        "print(word_list[:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyL062NuooAc"
      },
      "source": [
        "En utilisant un objet `nltk.FreqDist`, veuillez sélectioner les 500 mots les plus fréquents (vous pourrez aussi faire varier ce nombre), dans une liste appelée `vocabulary`.  À votre avis, est-ce une bonne idée d'enlever les *stopwords* de cette liste pour construire les traits ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jhDjay1eooAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad92cab7-a0d7-4ba1-b836-91d66dbaacd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('interest', 2066), ('rates', 652), ('that', 418), ('interests', 357), ('said', 231), ('million', 210), ('from', 200), ('with', 194), ('will', 183), ('rate', 167), ('company', 154), ('have', 154), ('which', 140), ('would', 133), ('bonds', 122), ('payments', 120), ('lower', 114), ('about', 94), ('they', 87), ('debt', 87), ('other', 87), ('also', 86), ('their', 85), ('market', 81), ('high', 80), ('because', 77), ('higher', 75), ('than', 74), ('after', 73), ('federal', 71), ('bank', 71), ('more', 70), ('short', 70), ('this', 64), ('billion', 64), ('foreign', 62), ('income', 60), ('some', 59), ('dollar', 57), ('been', 57), ('year', 56), ('stock', 55), ('general', 54), ('were', 54), ('annual', 53), ('says', 53), ('1989', 52), ('banks', 51), ('shares', 49), ('group', 49)]\n"
          ]
        }
      ],
      "source": [
        "# Veuillez répondre ici à la question.\n",
        "words_freq = nltk.FreqDist(list(word for word in word_list if len(word) > 3))\n",
        "vocabulary = words_freq.most_common(500)\n",
        "print(vocabulary[:50])\n",
        "\n",
        "# Oui c'est une bonne idée : les stopwords seront probablement présent dans les mêmes proportions dans tous les sens du mot. \n",
        "# Ils prennent donc la place de mots plus spécifiques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRc1PnMYooAd"
      },
      "source": [
        "Veuillez maintenant créer l'ensemble total de données formatées, en convertissant chaque phrase contenant une occurrence de *interest* à un dictionnaire de traits/valeurs (suivi du numéro du sens), comme exemplifié au début de cette section 3B.  Cet ensemble sera appelé `items_with_features_B`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "5YSQKHl0ooAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8938a364-efdf-48e6-dbb7-5ab9f19a5f4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2368\n",
            "[{'interest': False, 'rates': False, 'that': False, 'interests': True, 'said': False, 'million': True, 'from': False, 'with': False, 'will': False, 'rate': False, 'company': False, 'have': False, 'which': False, 'would': False, 'bonds': False, 'payments': False, 'lower': False, 'about': False, 'they': False, 'debt': False, 'other': False, 'also': False, 'their': False, 'market': False, 'high': False, 'because': False, 'higher': False, 'than': False, 'after': False, 'federal': False, 'bank': False, 'more': False, 'short': False, 'this': False, 'billion': False, 'foreign': False, 'income': False, 'some': False, 'dollar': False, 'been': False, 'year': False, 'stock': True, 'general': False, 'were': False, 'annual': False, 'says': False, '1989': False, 'banks': False, 'shares': True, 'group': False, 'minority': False, 'investors': False, 'corp': False, 'companies': False, 'loans': False, 'reserve': False, 'inflation': False, 'last': False, 'could': False, 'securities': False, 'buying': False, 'below': False, 'levels': False, 'shortterm': False, 'when': False, 'years': False, 'government': False, 'while': False, 'only': False, 'stocks': False, 'financial': False, 'down': False, 'funds': False, 'much': False, 'west': False, 'cash': False, 'increase': False, 'since': False, 'rise': False, 'over': False, 'sold': False, 'current': False, 'before': False, 'principal': False, 'business': False, 'economy': False, 'make': False, 'there': False, 'prices': False, 'profit': False, 'equity': False, 'guide': False, 'price': False, 'plus': False, 'traders': False, 'decline': False, 'rose': False, 'pursue': False, 'first': False, 'loan': False, 'money': False, 'week': False, 'fall': False, 'investor': False, 'best': False, 'including': False, 'sell': False, 'public': False, 'issues': False, 'japanese': False, 'such': False, 'accrued': False, 'concern': False, 'paid': False, 'certain': False, 'notes': False, 'many': False, 'taxes': False, 'earnings': False, 'central': False, 'trading': False, 'economic': False, 'time': False, 'those': False, 'october': False, 'month': False, 'products': False, 'acquire': False, 'capital': False, 'bond': False, 'sale': False, 'expressed': False, 'strong': False, 'payment': False, 'national': False, 'exchange': False, 'among': False, 'costs': False, 'american': False, 'move': False, 'stake': False, 'domestic': False, 'german': False, 'unit': False, 'further': False, 'amount': False, 'most': False, 'longterm': False, 'british': False, 'failed': False, 'international': False, 'total': False, 'real': False, 'japan': False, 'expected': False, 'should': False, 'both': False, 'bundesbank': False, 'might': False, 'continued': False, 'recent': False, 'drop': False, 'investments': False, 'making': False, 'yesterday': False, 'expense': False, 'share': True, 'despite': False, 'months': False, 'value': False, 'remain': False, 'speculation': False, 'growth': False, 'raise': False, 'boost': False, 'purchase': False, 'between': False, 'estate': False, 'germany': False, 'still': False, 'york': False, 'board': False, 'shareholders': False, 'holding': False, 'cost': False, 'treasury': False, 'average': False, 'investment': False, 'reported': False, 'raised': False, 'next': False, 'industrial': False, 'rising': False, 'agreed': False, 'insurance': False, 'what': False, 'through': False, 'property': False, 'growing': False, 'pressure': False, 'news': False, 'sept': False, 'already': False, 'increases': False, 'three': False, 'where': False, 'quarter': False, 'markets': False, 'must': False, 'major': False, 'reduce': False, 'include': False, 'september': False, 'issue': False, 'point': False, 'small': False, 'controlling': False, 'president': False, 'series': False, 'help': False, 'likely': False, 'resigned': False, 'under': False, 'jaguar': False, 'bill': False, 'back': False, 'into': False, 'analysts': False, 'soon': False, 'maker': False, 'until': False, 'then': False, 'policy': False, 'trade': False, 'well': False, 'friday': False, 'continue': False, 'during': False, 'keep': False, 'fell': False, 'helped': False, 'mortgage': False, 'currency': False, 'operating': False, 'these': False, 'increased': False, 'transaction': False, 'third': False, 'paying': False, 'european': False, 'possible': False, 'june': False, 'fees': False, 'open': False, 'being': False, 'services': False, 'little': False, 'percentage': False, 'options': False, 'return': False, 'changes': False, 'bring': False, 'revenue': False, 'credit': False, 'allow': False, 'businesses': False, 'generally': False, 'level': False, 'state': False, 'though': False, 'assets': False, 'very': False, 'country': False, 'voting': False, 'increasing': False, 'against': False, 'another': False, 'dealers': False, 'coming': False, 'monday': False, 'less': False, 'concerns': False, 'whose': False, 'corporate': False, 'half': False, 'francs': False, 'part': False, 'even': False, 'declined': False, 'issued': False, 'yield': False, '1988': False, 'expenses': False, 'number': False, 'take': False, 'enough': False, 'relatively': False, 'industry': False, 'showed': False, 'bought': False, 'sales': False, 'limited': False, 'management': False, 'tuesday': False, 'conflict': False, 'around': False, 'does': False, 'firm': False, 'personal': False, 'protect': False, 'depreciation': False, 'dividends': False, 'working': False, 'selling': False, 'zerocoupon': False, 'currently': False, 'thursday': False, 'finance': False, 'added': False, 'several': False, 'holds': False, 'show': False, 'expects': False, 'slowing': False, 'senior': False, 'them': False, 'yields': False, 'result': False, 'demand': False, 'attracted': False, 'instead': False, 'pound': False, 'based': False, 'offer': False, 'flow': False, 'mccaw': False, 'contracts': False, 'base': False, 'takeover': False, 'weeks': False, 'nearly': False, 'junk': False, 'allowing': False, 'meeting': False, 'executive': False, 'caused': False, 'agreement': False, 'fears': False, 'owns': False, 'raising': False, 'europe': False, 'chairman': False, 'made': False, 'holdings': False, 'probably': False, 'date': False, 'charges': False, 'additional': False, 'great': False, 'united': False, 'gold': False, 'times': False, 'later': False, 'least': False, 'position': False, 'banking': False, 'system': False, 'just': False, 'britain': False, 'people': False, 'receive': False, 'lack': False, 'trust': False, 'reduced': False, 'losses': False, 'fund': False, 'campeau': False, 'reason': False, 'filing': False, 'held': False, 'operations': False, 'received': False, 'consumer': False, 'without': False, 'large': False, 'falling': False, 'acquired': False, 'option': False, 'points': False, 'managers': False, 'bankers': False, 'debentures': False, 'again': False, 'customers': False, 'thus': False, 'rather': False, 'margin': False, 'ford': False, 'going': False, 'although': False, 'citicorp': False, 'includes': False, 'better': False, 'profits': False, 'congress': False, 'administration': False, 'prime': False, 'remains': False, 'almost': False, 'plans': False, 'however': False, 'western': False, 'proposal': False, 'paper': False, 'political': False, 'report': False, 'metromedia': False, 'cellular': False, 'effective': False, 'remaining': False, 'save': False, 'earlier': False, 'holders': False, 'offset': False, 'long': False, 'sharply': False, 'officials': False, 'second': False, 'appreciation': False, 'partners': False, 'special': False, 'gains': False, 'acquiring': False, 'resources': False, 'china': False, 'world': False, 'expect': False, 'declining': False, 'july': False, 'right': False, 'same': False, 'spring': False, 'spending': False, 'whether': False, 'borrowing': False, 'believe': False, 'important': False, 'venture': False, 'wednesday': False, 'future': False, 'service': False, 'program': False, 'weekly': False, 'security': False, 'ownership': False, 'deal': False, 'consumers': False, 'especially': False, 'ahead': False, 'french': False, 'institutional': False, 'building': False, 'days': False, 'come': False, 'official': False, 'according': False, 'research': False, 'above': False, 'monetary': False, 'australian': False, 'technology': False, 'like': False, 'represent': False, 'addition': False, 'slightly': False, 'defense': False, 'soviet': False, 'taking': False, 'period': False, 'priced': False, 'recently': False, 'operator': False, 'lost': False, 'data': False, 'acquisition': False, 'chief': False, 'majority': False, 'used': False, 'existing': False, '1987': False, 'things': False, 'leave': False, 'margins': False, 'five': False, 'keeping': False, 'given': False, 'protecting': False, 'resulting': False, 'particularly': False, 'primarily': False}, '5']\n"
          ]
        }
      ],
      "source": [
        "items_with_features_B = []\n",
        "# Veuillez répondre ici à la question.\n",
        "def extract_feature_b(sentences, window_size, vocabulary):\n",
        "  featured_items = []\n",
        "  windowed_sentences = list(extract_window_words(sentence, window_size) for sentence in sentences)\n",
        "  for sentence in windowed_sentences:\n",
        "    traits = {}\n",
        "    index_word = get_index_sens(sentence)\n",
        "    [word_interest, sens_nb] = sentence[index_word].split(\"_\")\n",
        "    sentence[index_word] = word_interest\n",
        "    for voc in vocabulary:\n",
        "      voc_word = voc[0]\n",
        "      traits[voc_word] = voc_word in sentence\n",
        "    featured_items.append([traits, sens_nb])\n",
        "  return featured_items\n",
        "\n",
        "items_with_features_B = extract_feature_b(sentences, 10, vocabulary)\n",
        "\n",
        "print(len(items_with_features_B))\n",
        "print((items_with_features_B[50][:50]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpOWdmB1ooAe"
      },
      "source": [
        "Comme dans la section 3A, veuillez créer maintenant deux sous-ensembles de `items_with_features_B` appelés `iwf_B_train` (80% des items) et `iwf_B_test` (20% des items), avec une sélection aléatoire mais stratifiée."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "w2D_u7KCooAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac25ce5-d8a8-4a79-a78c-570d6f8051fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1891   477\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Veuillez répondre ici à la question.\n",
        "(iwf_B_train, iwf_B_test) = make_train_test(items_with_features_B)\n",
        "\n",
        "print(len(iwf_B_train), ' ', len(iwf_B_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uum8yLTwooAf"
      },
      "source": [
        "Comme pour la section 3A, veuillez créer une instance de `NaiveBayesClassifier`, l'entraîner sur `iwf_B_train` et la tester sur `iwf_B_train`.  Veuillez obtenir le score de ce classifieur.  En expérimentant avec différentes largeurs de fenêtres et tailles du vocabulaire, quel est le meilleur score que vous obtenez, et comment se compare-t-il avec les précédents ?  Quels sont les traits les plus informatifs ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "L4HrnTZpooAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "172d591c-9515-44cd-9b13-b6187e672b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8846960167714885\n",
            "Most Informative Features\n",
            "                    deal = True                2 : 6      =    111.3 : 1.0\n",
            "                congress = True                2 : 6      =    111.3 : 1.0\n",
            "                    best = True                4 : 6      =    105.1 : 1.0\n",
            "                   rates = True                6 : 1      =     96.6 : 1.0\n",
            "                  pursue = True                3 : 5      =     71.1 : 1.0\n",
            "                interest = False               3 : 1      =     67.8 : 1.0\n",
            "                national = True                2 : 6      =     66.8 : 1.0\n",
            "                although = True                2 : 6      =     66.8 : 1.0\n",
            "                   might = True                2 : 6      =     61.9 : 1.0\n",
            "                 special = True                3 : 6      =     56.7 : 1.0\n",
            "               interests = True                3 : 1      =     52.7 : 1.0\n",
            "                   short = True                5 : 6      =     48.5 : 1.0\n",
            "                  almost = True                2 : 6      =     47.7 : 1.0\n",
            "               political = True                2 : 6      =     47.7 : 1.0\n",
            "               expressed = True                1 : 6      =     47.4 : 1.0\n"
          ]
        }
      ],
      "source": [
        "from nltk.classify import naivebayes \n",
        "# Veuillez répondre ici à la question.\n",
        "classifier_B = nltk.NaiveBayesClassifier.train(iwf_B_train)\n",
        "print(nltk.classify.accuracy(classifier_B, iwf_B_test))\n",
        "\n",
        "classifier_B.show_most_informative_features(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvy2LLROooAf"
      },
      "outputs": [],
      "source": [
        "# Veuillez recopier ici en conclusion les scores des quatre \n",
        "# expériences, pour pouvoir les comparer d'un coup d'oeil.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzZ7BjUxooAg"
      },
      "source": [
        "## Fin du laboratoire\n",
        "\n",
        "Merci de nettoyer votre feuille, exécuter une dernière fois toutes les instructions, sauvegarder le résultat, et soumettre le *notebook* sur Cyberlearn."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TAL_Labo5_3MethodesWSD_student.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}