{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAL Labo 1d : application au français\n",
    "\n",
    "**Objectifs**\n",
    "\n",
    "Cette dernière partie (1d) du Labo1 reprend les instructions précédentes et les applique à des textes en français.  Il s'agit d'une part de gérer correctement l'encodage de l'alphabet avec les caractères spécifiques au français, et d'autre part de voir comment les outils NLTK gèrent une langue différente de l'anglais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Données proposées**\n",
    "\n",
    "* http://www.gutenberg.org/files/19040/19040-8.txt : livre en hongrois, ISO-8859-2 (latin2)\n",
    "* http://www.gutenberg.org/files/41211/41211-8.txt : livre en français, ISO-8859-1 (latin1)\n",
    "* http://www.gutenberg.org/files/28049/28049-0.txt : livre en polonais, UTF-8 (en Python noté 'utf-8' avec tiret) \n",
    "\n",
    "Tout d'abord, enregistrez ces fichiers depuis les URLs indiquées sur votre ordinateur (une seule consultation de chaque URL), avec l'encodage correct indiqué ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "from urllib import request\n",
    "import matplotlib.pyplot \n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline\n",
    "\n",
    "#Create direcorties if they do not exists\n",
    "if not os.path.exists('Texts'):\n",
    "    os.makedirs('Texts')\n",
    "if not os.path.exists('Data'):\n",
    "    os.makedirs('Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "hongrois = {'url'     :\"http://www.gutenberg.org/files/19040/19040-8.txt\",\n",
    "            'encoding': \"latin2\",\n",
    "            'filename': \"./Data/sample_hongrois\",\n",
    "            'start'   : \"*** START OF THIS PROJECT GUTENBERG EBOOK TAKÁTS SÁNDOR SZALAI ***\",\n",
    "            'end'     : \"*** END OF THIS PROJECT GUTENBERG EBOOK TAKÁTS SÁNDOR SZALAI ***\"}\n",
    "francais = {'url'     :\"https://www.gutenberg.org/files/41211/41211-8.txt\",\n",
    "            'encoding': \"latin1\",\n",
    "            'filename': \"./Data/sample_francais\",\n",
    "            'start'   : \"*** START OF THIS PROJECT GUTENBERG EBOOK LA COMÉDIE HUMAINE VOLUME I ***\",\n",
    "            'end'     : \"*** END OF THIS PROJECT GUTENBERG EBOOK LA COMÉDIE HUMAINE VOLUME I ***\"}\n",
    "polonais = {'url'     :\"https://www.gutenberg.org/files/28049/28049-0.txt\",\n",
    "            'encoding': \"utf-8\",\n",
    "            'filename': \"./Data/sample_polonais\",\n",
    "            'start'   : \"*** START OF THIS PROJECT GUTENBERG EBOOK BALADY I ROMANSE ***\",\n",
    "            'end'     : \"*** END OF THIS PROJECT GUTENBERG EBOOK BALADY I ROMANSE ***\"}\n",
    "all_documents = [hongrois, francais, polonais]\n",
    "\n",
    "for document in all_documents:\n",
    "    response = request.urlopen(document['url'])\n",
    "    html = response.read().decode(document['encoding'])\n",
    "    filename = document['filename'] + '.txt'\n",
    "    if os.path.exists(filename): \n",
    "        os.remove(filename)\n",
    "    fd = open(filename, 'a', encoding=document['encoding'])\n",
    "    raw = BeautifulSoup(html).get_text()\n",
    "    fd.write(raw)\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant `open(filename, encoding='XXX', errors='replace')`, lisez dans une variable de type chaîne le contenu de chaque fichier, en essayant pour chacun des trois encodages (ou jeux de caractères) indiqués ci-dessus (latin1, latin2, ou utf-8).  Affichez un fragment de texte (100 à 200 caractères) et observez les différences, et si la lecture semble correcte ou non (vous aurez à remplir un tableau systématique plus bas).  Que se passe-t-il si vous n'indiquez aucun encodage lors de la lecture ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fichier : ./Data/sample_hongrois\n",
      "encoding:latin1\n",
      "mertetése, by Angyal Dávid\n",
      "\n",
      "\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "\n",
      "almo\n",
      "encoding:latin2\n",
      "mertetése, by Angyal Dávid\n",
      "\n",
      "\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "\n",
      "almo\n",
      "encoding:utf-8\n",
      "mertet�se, by Angyal D�vid\n",
      "\n",
      "\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "\n",
      "almo\n",
      "fichier : ./Data/sample_francais\n",
      "encoding:latin1\n",
      "noré de Balzac\n",
      "\n",
      "\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "\n",
      "almost no restri\n",
      "encoding:latin2\n",
      "noré de Balzac\n",
      "\n",
      "\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "\n",
      "almost no restri\n",
      "encoding:utf-8\n",
      "nor� de Balzac\n",
      "\n",
      "\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "\n",
      "almost no restri\n",
      "fichier : ./Data/sample_polonais\n",
      "encoding:latin1\n",
      "of anyone anywhere at no cost and with\n",
      "\n",
      "almost no restrictions whatsoever.  You may copy it, give it\n",
      "encoding:latin2\n",
      "of anyone anywhere at no cost and with\n",
      "\n",
      "almost no restrictions whatsoever.  You may copy it, give it\n",
      "encoding:utf-8\n",
      " anyone anywhere at no cost and with\n",
      "\n",
      "almost no restrictions whatsoever.  You may copy it, give it a\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "encodings = ['latin1', 'latin2', 'utf-8']\n",
    "for document in all_documents:\n",
    "    print('fichier : ' + document['filename'])\n",
    "    for encoding in encodings:\n",
    "        filename = document['filename'] + '.txt'\n",
    "        fd = open(filename, encoding=encoding, errors='replace')\n",
    "        print('encoding:' + encoding)\n",
    "        text = fd.read()\n",
    "        print(text[100:200])\n",
    "        fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez résumer vos conclusions dans le tableau suivant.  Pour chaque fichier, indiquez si la lecture faite avec les différents encodages est correcte ou non, et si non, indiquez au moins un caractère qui est erronné.\n",
    "          \n",
    "| Fichier (format) / Lecture :  | utf8  | latin1  | latin2  | none |\n",
    "| ----------------------------- | ----- | ------- | ------- | ---- |\n",
    "| book_fr (original : latin1)   | non(é)| oui     | non(è)  | oui  |\n",
    "| book_hu (original : latin2)   | non(á)| oui     | oui     | oui  |\n",
    "| book_pl (original : utf-8)    | oui   | oui     | oui     | oui  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que se passe-t-il si vous n'utilisez pas l'option `errors='replace'` ?  Vous pouvez réutiliser le tableau pour indiquer les réponses à la question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une erreur python est lancée quand un caractère inconnu est rencontré"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Écrivez maintenant chaque texte dans un fichier au format utf-8**, en supprimant comme précédemment les parties initiales et finales qui proviennent du site *Project Gutenberg* et qui ne font pas partie du texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "for document in all_documents:\n",
    "    utf8_filename = document['filename'] + '_utf8.txt'\n",
    "    filename = document['filename'] + '.txt'\n",
    "    if os.path.exists(utf8_filename): \n",
    "        os.remove(utf8_filename)\n",
    "    fd_utf8 = open(utf8_filename, 'a', encoding='utf-8')\n",
    "    fd = open(filename, 'r', encoding=document['encoding'])\n",
    "    raw = fd.read()\n",
    "    fd.close()\n",
    "    start_index = raw.index(document['start']) + len(document['start'])\n",
    "    end_index = raw.index(document['end']) -1\n",
    "    book_text = raw[start_index:end_index]\n",
    "    fd_utf8.write(book_text)\n",
    "    fd_utf8.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation en phrases et en mots avec NLTK, selon la langue\n",
    "\n",
    "L'objectif de cette étape est d'observer la meilleure solution pour la segmentation en phrases et la tokenization du texte français avec les outils NLTK et l'option `language='french'`.\n",
    "\n",
    "Tout d'abord, faire `import nltk` et charger le texte français dans une variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "filename = francais['filename'] + '_utf8.txt'\n",
    "fd = open(filename, 'r', encoding='utf-8')\n",
    "book_francais = fd.read()\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Segmentation en phrases.** Téléchargez d'abord le segmenteur de phrases `Punkt` pour le français avec la commande suivante.  Appliquez ce segmenteur au texte français.  Appliquez-lui également le tokenizer par défaut de NLTK, d'abord sans l'option `language=\"french\"`, puis avec elle.  Affichez et comparez le nombre de phrases obtenues dans chacun des trois cas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer_fr = nltk.data.load(resource_url = 'tokenizers/punkt/french.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9510\n",
      "9452\n",
      "9510\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it\n",
    "punkt_tokens = sent_tokenizer_fr.tokenize(book_francais)\n",
    "default_nltk_tokens = nltk.sent_tokenize(book_francais)\n",
    "fr_nltk_tokens = nltk.sent_tokenize(book_francais, language=\"french\")\n",
    "print(len(punkt_tokens))\n",
    "print(len(default_nltk_tokens))\n",
    "print(len(fr_nltk_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parmi les trois segmentations ci-dessus, deux génèrent le même nombre de phrases noté *N*.  Testez si pour chaque *i* compris entre 1 et *N*, les *i*èmes phrases de chaque segmentation ont la même longueur en caractères et en mots, et affichez les valeurs de *i* pour lesquelles ces valeurs sont différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "for i in range(len(punkt_tokens)):\n",
    "    if len(punkt_tokens[i]) != len(fr_nltk_tokens[i]) or len(punkt_tokens[i].split(' ')) !=  len(fr_nltk_tokens[i].split(' ')):\n",
    "        print(\"Les valeurs diffèrent en :\" + i)\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization** (segmentation de chaque phrase en mots).  Comparer le tokenizer par défaut de NLTK (avec l'option `language='french'`) avec le tokenizer basé sur des expressions régulières fourni ci-après.  \n",
    "* Leurs résultats sont-ils identiques ?\n",
    "* Comparer les tokenizations de plusieurs phrases et indiquer quelle méthode est plus adaptée.\n",
    "* Comparez le nombre total de tokens obtenus par chaque méthode.\n",
    "* En regardant la documentation de `nltk.word_tokenize`, que pensez-vous de son adaptation au français ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "nltk_fr_word_tokens = nltk.word_tokenize(book_francais, language='french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tokenizer suivant utilise des expressions régulières.  Il est adapté de http://fabienpoulard.info/post/2008/03/05/Tokenisation-en-mots-avec-NLTK. Il peut être invoqué avec la méthode `tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer_fr = nltk.RegexpTokenizer(r'''(?x)\n",
    "          \\d+(?:\\.\\d+)?\\s*%   # les pourcentages\n",
    "        | \\w'               # les contractions d', l', ...\n",
    "        | \\w+               # les mots pleins\n",
    "        | [^\\w\\s]           # les ponctuations\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "nltk_regexp_word_tokens = word_tokenizer_fr.tokenize(book_francais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20616\n",
      "['pas', 'été', 'modifiée', 'hormis', 'quelques', 'corrections', 'mineures', '.', \"L'orthographe\", 'a', 'été', 'conservée', '.', 'Seuls', 'quelques', 'mots', 'ont', 'été', 'modifiés', '.', 'La', 'liste', 'des', 'modifications', 'se', 'trouve', 'à', 'la', 'fin', 'du', 'texte', '.', 'OEUVRES', 'COMPLÈTES', 'DE', 'H', '.', 'DE', 'BALZAC', 'LA', 'COMÉDIE', 'HUMAINE', 'PREMIER', 'VOLUME', 'PREMIÈRE', 'PARTIE', 'ÉTUDES', 'DE', 'MOEURS', 'PREMIER']\n",
      "['version', 'originale', '.', 'La', 'ponctuation', \"n'\", 'a', 'pas', 'été', 'modifiée', 'hormis', 'quelques', 'corrections', 'mineures', '.', \"L'\", 'orthographe', 'a', 'été', 'conservée', '.', 'Seuls', 'quelques', 'mots', 'ont', 'été', 'modifiés', '.', 'La', 'liste', 'des', 'modifications', 'se', 'trouve', 'à', 'la', 'fin', 'du', 'texte', '.', 'OEUVRES', 'COMPLÈTES', 'DE', 'H', '.', 'DE', 'BALZAC', 'LA', 'COMÉDIE', 'HUMAINE']\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "print(len(nltk_fr_word_tokens) - len(nltk_regexp_word_tokens))\n",
    "w = 50\n",
    "print(nltk_fr_word_tokens[w:w + 50])\n",
    "print(nltk_regexp_word_tokens[w:w + 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dans les parties 1b et 1c, **veuillez écrire dans un fichier texte** appelé `book_fr.utf-8.tok.txt`le texte français tokenisé, avec des espaces entre les tokens et une phrase par ligne.  Utilisez la meilleure tokenization des deux précédentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Texts/book_fr.utf-8.tok.txt'\n",
    "if os.path.exists(filename): \n",
    "    os.remove(filename)\n",
    "fd = open(filename, 'a', encoding='utf8')\n",
    "# Please write your Python code below and execute it.\n",
    "word_tokenized_sentences = [word_tokenizer_fr.tokenize(sentence)for sentence in fr_nltk_tokens]\n",
    "for line in word_tokenized_sentences:\n",
    "    fd.write(\" \".join(line))\n",
    "    fd.write('\\n')\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin de la partie 1d et du Labo1\n",
    "Veuillez nettoyer autant que possible ce _notebook_, exécutez une dernière fois toutes les cellules pour obtenir les résultats demandés, et enregistrez le _notebook_.  Puis ajoutez-le dans une archive _zip_ avec les _notebook_ des parties 1b et 1c, et soumettez l'archive individuellement sur Cyberlearn (_Laboratoire 1_). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
